---
title: "p8105_hw5_sdc2157"
author: "Stephanie Calluori"
date: 2023-11-15
output: github_document
---

# Load packages and set seed
```{r packages, message = FALSE}
library(tidyverse)
library(readr)
library(rvest)
library(httr)
library(patchwork)

set.seed(1)
```

```{r setup, message = FALSE, echo = FALSE, results = FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Import, Clean, and Describe the Data

```{r case data, message = FALSE}
homicide_raw <- read_csv("data/homicide_data_copy.csv", col_names = TRUE, na = c("", "NA", "Unknown"))

homicide_clean <- homicide_raw |> 
  janitor::clean_names() |> 
  mutate(victim_age = as.numeric(victim_age)) |> 
  mutate(city_state = paste(city, state, sep = ",")) |> 
  mutate(disposition = recode(
    disposition,
    "Closed by arrest" = "solved",
    "Closed without arrest" = "unsolved",
    "Open/No arrest" = "unsolved")) |> 
  filter(city_state != "Tulsa,AL") 

prop_status <- homicide_clean |> 
  count(disposition) |> 
  mutate(prop = n / sum(n)*100)

```

The Washington Post gathered data on homicides in 50 large US cities to examine the number and characteristics of homicides that go solved vs unsolved. The dataset has `r nrow(homicide_clean)` rows and `r ncol(homicide_clean)` columns. Each row in the dataset is a case.

Variables include city and state name, details about the victim (race, age, sex), and case status. 

Of the `r nrow(homicide_clean)` cases, about `r round(prop_status[1,3])` percent are solved and `r round(prop_status[2,3])` percent are unsolved. Victim age was of class character, so it was converted to numeric.

## Total number of homicides across cities 

```{r total cases}
homicide_clean |> 
  count(city_state) |> 
  mutate(city_state = forcats::fct_reorder(city_state, n, .desc = TRUE)) |> 
  ggplot(aes(x = city_state, y = n)) + 
  geom_col() +
  labs(
    title = "Total number of homicides in each city",
    x = "city",
    y = "Total # of homicides"
  ) +
  theme(axis.text.x = element_text(angle=90, vjust=1, hjust=1))

```
Chicago has the highest total number of homicides. Following Chicago, Philadelphia, Houston, and Baltimore have the highest number of homicides.

## Analysis of unsolved homicides across cities

```{r total unsolved}
homicide_clean |> 
  filter(disposition == "unsolved") |> 
  count(city_state) |> 
  mutate(city_state = forcats::fct_reorder(city_state, n, .desc = TRUE)) |> 
  ggplot(aes(x = city_state, y = n)) + 
  geom_col() +
  labs(
    title = "Number of unsolved homicides in each city",
    x = "city",
    y = "# of unsolved homicides"
  ) +
  theme(axis.text.x = element_text(angle=90, vjust=1, hjust=1))

```
Chicago has the highest number of unsolved homicides. Following Chicago, Baltimore, Houston, and Detroit have the highest number of unsolved homicides.


```{r baltimore}
baltimore_df <- homicide_clean |> 
  filter(city == "Baltimore")

num_x <- baltimore_df |> 
  filter(disposition == "unsolved") |> 
  nrow()

n_sample <- nrow(baltimore_df)

output <- prop.test(x = num_x, n = n_sample)

output |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

```
In Baltimore, about 64.6% of homicides are unsolved. 95% Confidence interval: (62.8, 66.3)

Below, I created a function to extract the estimate and confidence interval for the proportion of unsolved homicides in each city.

```{r estimate CI function}
estimate_and_CI <- function(city_df) {
  
uns <- city_df |> 
  filter(disposition == "unsolved") |> 
  nrow()

sample_size <- nrow(city_df)

output <- prop.test(x = uns, n = sample_size)

output |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)
  
}

```

```{r unsolved dataset}
homicide_nest_df <- homicide_clean |> 
  select(city, everything()) |> 
  nest(.data = _, data = uid:city_state)

results <- homicide_nest_df |> 
  mutate(summary = map(data, estimate_and_CI)) |> 
  unnest(cols = summary) |> 
  select(city, estimate, conf.low, conf.high)


```

```{r proportion unsolved graph}
results |> 
  mutate(city = forcats::fct_reorder(city, estimate, .desc = TRUE)) |> 
  ggplot(aes(x = city, y = estimate)) +
  geom_point() +
  geom_errorbar(
    aes(x = city,
        ymin = conf.low,
        ymax = conf.high)
  ) +
  labs(
    title = "Estimate of the proportion of unsolved homicides in each city",
    x = "city",
    y = "estimate of proportion of unsolved homicides"
  ) +
  theme(axis.text.x = element_text(angle=90, vjust=1, hjust=1))


```
Chicago has the highest proportion of unsolved homicides. Aside from Chicago, there is noticeable variability in the proportion of unsolved homicides across the cities.

# Problem 2

## Import, tidy, and examine data from an RCT
In this RCT, 10 participants were assigned to the experimental group and 10 participants were assigned to the control group. Each participant was followed for 8 weeks, and observations were collected weekly.

```{r RCT load clean, message = FALSE}
participant_list <- list.files("data/rct_data", full.names = TRUE)

participant_df <- tibble(file_name = participant_list) |> 
  mutate(all_obs = (map(file_name, \(f) read_csv(file = f)))) 

participant_clean <- participant_df |> 
  unnest(all_obs) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "obs",
    names_prefix = "week_") |> 
  mutate(file_name = map(file_name, \(g) basename(g))) |> 
  unnest(file_name) |> 
  mutate(file_name = stringr::str_remove(file_name, ".csv")) |> 
  rename(id = file_name) |> 
  mutate(week = as.numeric(week)) |> 
  mutate(arm = stringr::str_sub(id, 1, 3)) |> 
  select(id, arm, week, obs)

```

```{r RCT graph}
participant_clean |> 
  ggplot(aes(x = week, y = obs)) +
  geom_point(aes(color = id)) +
  geom_line(aes(color = id)) +
  geom_smooth(se = FALSE) +
  facet_grid(~arm) +
  labs(title = "Observational values for participants over 8 weeks")
  
```

On average, participants in the experimental group presented with higher values for our outcome compared to participants in the control group over the course of 8 weeks. Over the 8 weeks, the average value of the outcome for participants in the control group was fairly steady while the average value of the outcome for participants in the experimental group increased.


# Problem 3

We will conduct a simulation to explore power in a one-sample two-sided t-test.

First, we create a dataframe creating the parameters for each of our models. Then, we generate 5000 datasets from each of the models. Next, we apply a one-sample t-test to each of the datasets. 
```{r simulation}
t_test <- function(x) {
  t = t.test(x = x, mu = 0, conf.level = 0.95)
  
  broom::tidy(t)
}


simul_df <- 
  expand_grid(
    sample_size = 30,
    true_mean_u = 0:6,
    iter = 1:5000
  ) |> 
  mutate(simul_dataset = map2(sample_size, true_mean_u, \(sa, tmu) rnorm(n = sa, mean = tmu, sd = 5)))


simul_df <- simul_df |> 
  mutate(summary = map(simul_dataset, \(si) t_test(x = si))) |> 
  unnest(summary) |> 
  select(true_mean_u, estimate, p.value)

```


```{r power and effect}
simul_df |> 
  select(true_mean_u, estimate, p.value) |> 
  mutate(null_rejected = case_when(
    p.value <= 0.05 ~ "Yes",
    p.value > 0.05 ~ "No")) |>
  group_by(true_mean_u) |>
  count(null_rejected) |> 
  mutate(prop = n / sum(n)) |> 
  filter(null_rejected == "Yes") |> 
  ggplot(aes(x = true_mean_u, y = prop)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Plot of power against mu",
    x = "mu",
    y = "power"
  )

```

In this graph, we see that as mu increases, the proportion of times the null hypothesis was rejected increases. This indicates that as effect size increases, power increases. When our effect size is large, we are able to better detect a false null hypothesis.


```{r mu graph}

rejected <- simul_df |> 
  select(true_mean_u, estimate, p.value) |> 
  filter(p.value <= 0.05) |> 
  group_by(true_mean_u) |> 
  summarize(avg_estimate_u = mean(estimate))

simul_df |> 
  select(true_mean_u, estimate) |> 
  group_by(true_mean_u) |> 
  summarize(avg_estimate_u = mean(estimate)) |> 
  ggplot(aes(true_mean_u, avg_estimate_u)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Plot of avg u_hat against mu",
    x = "mu",
    y = "average mu_hat",
    caption = "Black: All Samples, Purple: Only samples for which null was rejected"
  ) + 
  geom_point(data = rejected) +
  geom_line(data = rejected, color = "purple") 

```

When our hypothesized mu is in the range of 0 to 4, the average estimate of mu differs between all samples vs only samples for which the null was rejected. When mu exceeds 4, the average estimate of mu becomes similar.

When we look at the data for only samples for which the null was rejected, the discordance between the hypothesized values of mu (x axis) and the average estimates of mu (y axis) aligns with our expectations. When we reject a null hypothesis, we are saying that we have evidence that the true value of mu is different from our hypothesized value for mu. 

   



